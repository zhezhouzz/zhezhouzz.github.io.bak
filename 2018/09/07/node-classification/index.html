<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>node classification, intro | Zhou Zhe&#39;s Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Node Classification or Graph Embedding Inductive Representation Learning on Large Graphs.  Goals: Encode, Decode and Data Transformation Distance: In the classical algorithm, if we can define a “dista">
<meta name="keywords" content="ML paper-reading">
<meta property="og:type" content="article">
<meta property="og:title" content="node classification, intro">
<meta property="og:url" content="http://yoursite.com/2018/09/07/node-classification/index.html">
<meta property="og:site_name" content="Zhou Zhe&#39;s Home">
<meta property="og:description" content="Node Classification or Graph Embedding Inductive Representation Learning on Large Graphs.  Goals: Encode, Decode and Data Transformation Distance: In the classical algorithm, if we can define a “dista">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/embedding.jpeg">
<meta property="og:image" content="http://yoursite.com/images/DeepWalk.jpg">
<meta property="og:image" content="http://yoursite.com/images/Node2vec.jpg">
<meta property="og:image" content="http://yoursite.com/images/matrix-factorization.jpg">
<meta property="og:image" content="http://yoursite.com/images/convolution-locally-on-graph.jpg">
<meta property="og:image" content="http://yoursite.com/images/ranking-convolution-on-graph.jpg">
<meta property="og:image" content="http://yoursite.com/images/GraphSAGE-algo.jpg">
<meta property="og:image" content="http://yoursite.com/images/forward-of-graphSAGE.jpg">
<meta property="og:image" content="http://yoursite.com/images/error-function-graphSAGE.jpg">
<meta property="og:image" content="http://yoursite.com/images/minibatch-of-graphSAGE.jpg">
<meta property="og:updated_time" content="2018-09-25T20:57:01.666Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="node classification, intro">
<meta name="twitter:description" content="Node Classification or Graph Embedding Inductive Representation Learning on Large Graphs.  Goals: Encode, Decode and Data Transformation Distance: In the classical algorithm, if we can define a “dista">
<meta name="twitter:image" content="http://yoursite.com/images/embedding.jpeg">
  
    <link rel="alternate" href="/atom.xml" title="Zhou Zhe&#39;s Home" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhou Zhe&#39;s Home</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-node-classification" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/07/node-classification/" class="article-date">
  <time datetime="2018-09-07T16:22:41.000Z" itemprop="datePublished">2018-09-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      node classification, intro
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Node-Classification-or-Graph-Embedding"><a href="#Node-Classification-or-Graph-Embedding" class="headerlink" title="Node Classification or Graph Embedding"></a>Node Classification or Graph Embedding</h4><ul>
<li><a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">Inductive Representation Learning on Large Graphs</a>.</li>
</ul>
<h5 id="Goals-Encode-Decode-and-Data-Transformation"><a href="#Goals-Encode-Decode-and-Data-Transformation" class="headerlink" title="Goals: Encode, Decode and Data Transformation"></a>Goals: Encode, Decode and Data Transformation</h5><ul>
<li><p><strong>Distance</strong>: In the classical algorithm, if we can define a “distance” of two object, then we can classify them(such as K-means or others). For two high-dimensional vectors, the calculation of distance is easy, we have Euclidean distance or others. However, when the given data is a non-Euclidean data, especially a graph, we can not easily calculate the distance of two nodes. A reasonable definition of distance could be the length of a shortest route between two nodes. However, the complexity of calculating the shortest path between every two nodes is expensive. The traditional classification algorithm could not handle the non-Euclidean data.</p>
</li>
<li><p><strong>Embedding</strong>: In the view of math, a topology graph is a abstraction from a high-dimensional angle whose vertex is the origin of the coordinate(a graph with n node equal to a n-dimension object, which every node is located in one axis). To know this relation between the node of this angle, we do not need the information of the whole object. If we just project this object to a plane, we could also get the right distance. Converting a graph(is a high-dimensional object essentially) to a matrix(or a low-dimension vector) is called <em>Embedding</em>. Actually, the adjacency matrix is a method of embedding(because it loses a lot of information and the adjacency matrix itself is also complex).</p>
</li>
<li><p><img src="/images/embedding.jpeg" alt="embedding"></p>
</li>
<li><p><strong>Encode, Decode</strong>: The procedure for converting a graph to a matrix is called <em>encode</em>. We can also recover the graph by the encoded matrix, which is called <em>decode</em>. Then we can define a loss rate between the origin graph and “recovered graph”. If we use the <em>gradient descent</em> method to adjust the weight matrix to find the extremum of loss function, then we can find a reasonable encoder(because we would get a decode function, too. But it is useless at most of time).</p>
</li>
</ul>
<h5 id="From-NLP-to-Node-Analysis-Matrix-Factorization"><a href="#From-NLP-to-Node-Analysis-Matrix-Factorization" class="headerlink" title="From NLP to Node Analysis, Matrix Factorization"></a>From NLP to Node Analysis, Matrix Factorization</h5><ul>
<li><p>At first, the <em>Embedding</em> is used in NLP, which is called <em>word2vec</em>: <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a>. Word2vec use a <em>Skip-gram Model</em> which is a NN that has three layers: input, hidden and output. The input to hidden is the encoder, the hidden to output is the decoder. The forward of this NN is “encode + decode”, thus when we are training this NN, the encode is more and more reliable. After training, we extract the hidden layer, it is the resulting low-dimension matrix we want.</p>
</li>
<li><p>Then, this approach are used in the graph data which is called <em>DeepWalk</em>: <a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" target="_blank" rel="noopener">DeepWalk: Online Learning of Social Representations</a>, <em>node2vec</em>: <a href="https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf" target="_blank" rel="noopener">node2vec: Salable Feature Learning for Networks</a>. The different is the training data. In the NLP, the training data from M-gram, the traditional method. We first aggregate the adjacency word pairs(actually is the word pair which distance less than a hyper-parameter), and make sure the distance of represented vector of word pair is also close. In the graph, training data is the node pair which is in the neighborhood. But the neighborhood sample is more complex than NLP. The <em>DeepWalk</em> use sampling just merge <em>DFS</em> and <em>BFS</em>. <em>Node2vec</em> just improves the sampling.</p>
</li>
<li><p><img src="/images/DeepWalk.jpg" alt="DeepWalk"></p>
</li>
<li><p><img src="/images/Node2vec.jpg" alt="Node2vec"></p>
</li>
<li><p>the mathematical meaning of <em>node2vec</em> and <em>DeepWalk</em>. I have make a very simple explain before, and there is a paper(<a href="https://www.ijcai.org/Proceedings/15/Papers/299.pdf" target="_blank" rel="noopener">Network Representation Learning with Rich Text Information</a>) proof that <em>DeepWalk</em> is actually equivalent to <em>matrix factorization</em>. Let’s Image there is a <em>n x n</em> matrix _M_ correspond to a graph with _n_ nodes. If the value of <em>M(i,j)</em> is not zero, this indicate there is a path between node _i_ and node _j_ and the min length of the path is less than hyper-parameter _d_, which is just the depth of our <em>BNF</em> or <em>DNF</em>. If the <em>d = 1</em>, then the _M_ is our adjacency matrix. Then, this paper prove that <em>DeepWalk</em> just factorize this matrix, from <em>n x n</em> to <em>n x d</em>.</p>
</li>
<li><p><img src="/images/matrix-factorization.jpg" alt="matrix-factorization"></p>
</li>
</ul>
<h5 id="Graph-Convolutional-Networks-based-on-spectral"><a href="#Graph-Convolutional-Networks-based-on-spectral" class="headerlink" title="Graph Convolutional Networks(based on spectral)"></a>Graph Convolutional Networks(based on spectral)</h5><ul>
<li><p>If we have a graph structured of signals(a sequence of value), how can we find the hidden properties of these signals? The signals of the nodes are very important, but the relationship of the nodes is also meaningful.  Traditionally, machine learning handles this kind of problem by CNN, because convolution could <em>extract</em> hidden patterns of the data in the <em>neighborhood</em>. But how could we do convolution on a graph?</p>
</li>
<li><p>In the traditional convolution, we have a convolution <em>kernel</em>, which decides which information we should aggregate. These <em>kernels</em> are predefined, as during matrix convolution every <em>node</em> has 8 nodes surrounding it. However, the convolution kernels of the graph should come from the graph structure itself. If a node is 1-step nearby to you, you should aggregate the information of it. If a node is far from you, the corresponding value of the convolution kernel should be multiplied by zero.</p>
</li>
<li><p>The basic method to generate the kernel is <em>Graph Fourier Transform</em> which converts a adjacency matrix to a <em>kernel</em>. If the adjacency matrix is a positive symmetry matrix, it can be represented by the product between eigen-diag matrix and eigenvector. Then, the eigenvalues indicate the component of vector. In other words, if this node is close to one eigenvector this positive symmetry matrix is the <em>kernel</em> we want(more details in <a href="https://arxiv.org/pdf/1606.09375.pdf" target="_blank" rel="noopener">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>). <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a> introduces a simple and reasonable way to calculate the <em>kernel</em> from adjacency matrix. As we know, the <em>ReLU</em> part is the “encoder”, and <em>softmax</em> is the decoder.</p>
</li>
<li><p>This paper also use a <em>semi-supervised method</em> to train the NN. Not all the nodes have a label(such as, this node belongs to class A). Then, the error function only calculates the error of the labeled node. As the whole nodes shared the same weight matrix, we could also get the correct result.</p>
</li>
</ul>
<h5 id="Graph-Convolutional-Networks-based-on-spatial"><a href="#Graph-Convolutional-Networks-based-on-spatial" class="headerlink" title="Graph Convolutional Networks(based on spatial)"></a>Graph Convolutional Networks(based on spatial)</h5><ul>
<li><p>When we try to calculate a convolution, we actually first split the data into different parts, then times the <em>kernel</em> with each part and finally merge. So if we can find a reasonable way to split the graph into different but overlapping parts, the convolution in the graph is almost complete. Here we introduce two ways.</p>
</li>
<li><p>In the <a href="https://arxiv.org/pdf/1312.6203.pdf" target="_blank" rel="noopener">Spectral Networks and Deep Locally Connected Networks on Graphs</a>, the author uses a way to do convolution on graph-like data: each node has independent information vectors. First, the author uses the metric designed by <a href="http://robotics.stanford.edu/~ang/papers/nips11-SelectingReceptiveFields.pdf" target="_blank" rel="noopener">Selecting Receptive Fields in Deep Networks</a>, which is a statistical method to define the distance of two vectors. Then, we get a similarity matrix about each node pair. Then we could use a filter(in this paper, this is a threshold) to divide a neighborhood for each node. If there are some neighborhoods almost the same, then we randomly choose one. By using these nodes to neighborhoods mapping, we get a set of different but overlapping parts(if the number of nodes in the neighborhoods are not equal, add null nodes). If the neighborhood _NA_ and neighborhood _NB_ shared the same point from the original graph, then we could say there is a edge between _NA_ and _NB_. The neighborhoods actually have a graph-like structure. So the convolution could be recursive.</p>
</li>
<li><p>The disadvantage is that this adds a lot of null nodes, so the neighborhoods with less nodes would have higher priority. And the calculation complexity of <em>metric</em> make it not scalable to large graphs.</p>
</li>
<li><p><img src="/images/convolution-locally-on-graph.jpg" alt="convolution-locally-on-graph"></p>
</li>
<li><p>The author of <a href="https://arxiv.org/pdf/1605.05273.pdf" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a> provides another way. First, the author believes that not every node in the graph is really important; he suggests to choose the most important _n_ nodes, and do convolution on the neighborhoods of these nodes. So we should try to guarantee these _n_ neighborhood include all the nodes in the graph. The author ranks the nodes in the graph by the sum of distances from it to all other nodes. This approach is called <em>ranking</em>. After <em>ranking</em>, we extract the first _n_ nodes. For each node, he uses a <em>BFS</em> to construct the neighborhood. Then he <em>ranks</em> the neighborhoods, and sorts the neighborhoods. The sorted vector could be sent to do convolution now.</p>
</li>
<li><p><img src="/images/ranking-convolution-on-graph.jpg" alt="ranking-convolution-on-graph"></p>
</li>
<li><p>The complexity of this approach is also unscalable.</p>
</li>
</ul>
<h5 id="What-GraphSAGE-Do"><a href="#What-GraphSAGE-Do" class="headerlink" title="What GraphSAGE Do?"></a>What <em>GraphSAGE</em> Do?</h5><ul>
<li><p><strong>Shortage of old work</strong>:</p>
<ul>
<li>Focused on embedding nodes from a single fixed graph, but many real-world applications require embedding to be quickly generated for unseen nodes, or entirely new (sub)graphs. Both <em>Matrix Factorization</em> and <em>Graph Convolutional Networks</em> need a fixed abstraction of the adjacency matrix.</li>
<li>The weight matrices are not relevant between nodes in <em>Matrix Factorization</em>.</li>
<li><em>Graph-Based Neural Network</em> could not be used in large scale data. Because this approach requires to learn node degree-specific weight matrices does not scale to large graphs with wide node degree distributions.</li>
<li><em>Graph Convolutional Networks</em> is transductive node classification and does not naturally generalize to unseen nodes.</li>
</ul>
</li>
<li><p>If we cannot determine the graph structure because the structure is active, we should drop the idea of <em>matrix factor</em> or <em>convolution kernel</em>. We could try <em>DeepWalk</em>, just sample the neighborhood by iterating the graph. But there is a disadvantage of <em>DeepWalk</em>; the degree of graph is diverse. For the <em>DFS</em>, if node has large degrees it would generate more training data than others. This generates a kind of bias. To reduce this bias, we set a <em>specified degree</em> as a hyperparameter to restrict every degree of nodes to be the same.</p>
</li>
<li><p><img src="/images/GraphSAGE-algo.jpg" alt="GraphSAGE-algo"></p>
</li>
<li><p><img src="/images/forward-of-graphSAGE.jpg" alt="forward-of-graphSAGE"></p>
</li>
<li><p>Unlike <em>GCN</em>, the <em>GraphSAGE</em> is fully unsupervised. <em>GraphSAGE</em> aggregates the information of neighbor nodes, and every node shares the same weight matrix. <em>GraphSAGE</em> trusts that the information of nodes nearby is also similar. So, after forwarding of NN, the result we get is the low-dimension matrix. And we have the guarantee that each vector of node is close to its neighborhood.</p>
</li>
<li><p><img src="/images/error-function-graphSAGE.jpg" alt="error-function-graphSAGE"></p>
</li>
<li><p><strong>Aggregator</strong>. compare with <em>Graph Convolutional Networks</em>. If the aggregator is the <em>mean aggregator</em>, and the <em>GraphGAGE</em> does not distinguish old aggregation result and new information(the concatenation operation), they are the same. Another reasonable aggregator is <em>max pooling</em>(also the final choosing of the author), because it is both symmetric and trainable. Each neighbor’s vector is independently fed through a fully-connected neural network and then pooled. Then the model effectively captures different aspects of the neighborhood set.</p>
</li>
<li><p><strong>Minibatch</strong>. <em>GraphSAGE</em> also uses the minibatch method. For the set of all nodes in the graph, we can find a subset of the nodes: if we start from the nodes of the subset, we can go to every node in the graph in only one step. When we iterate this procedure, then we can get set list with size _d_, where _d_ is the max step we need to walk. Then, the difference set of one set of set list and its successor is the <em>batch of nodes</em> we want.</p>
</li>
<li><p><img src="/images/minibatch-of-graphSAGE.jpg" alt="minibatch-of-graphSAGE"></p>
</li>
<li><p><strong>Weisfeiler-Lehman Isomorphism Test</strong>. This algorithm is a graph embedding version of <a href="http://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf" target="_blank" rel="noopener">Weisfeiler-Lehman Graph Kernels</a>, which is simplely a 1-dim version of <code>Weisfeiler-Lehman Isomorphism Test</code>, also called <code>naive vertex refinement</code>. There are two significant difference between <code>naive vertex refinement</code> and <code>graphSAGE</code>. First, graphSAGE do the embedding(refine the information of all nodes from the topographic structure and the node lables), not to test the isomorphism. Second, graphSAGE use the NN instead of the hash function in the <code>naive vertex refinement</code> and omits the sortting operation(the aggregators are all symmetric). If we want to test two graph, we must garantee the embedding vector is identical. which means that <code>forall (v_set1 v_set2 : list vertex), Embedding v_set1 = Embedding v_set2 &lt;-&gt; (forall (v1 v2: vertex), In v1 v_set1 -&gt; In v2 v_set2 -&gt; v1 = v2)</code>. WL use the hash function, because the hash function could provide this property. However, if we do not sort the vertexs in the vertex set, the <code>forall (v1 v2: vertex), In v1 v_set1 -&gt; In v2 v_set2 -&gt; v1 = v2</code> would have <code>(length v_set1) * (length v_set2)</code> terms. This could cause the vector after hash very loog. So we sort the vertexs in the set first to reduce the length of vector after WL.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/07/node-classification/" data-id="cjmi7bqwi000hus4nsn5huzvw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML-paper-reading/">ML paper-reading</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/09/18/talk-with-prof-suresh-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          talk-with-prof-suresh-3
        
      </div>
    </a>
  
  
    <a href="/2018/09/04/talk-with-Prof-Suresh-2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">talk-with-Prof.Suresh-2</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML-paper-reading/">ML paper-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Prof-Suresh/">Prof.Suresh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sdh/">sdh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/summary/">summary</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/suresh/">suresh</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML-paper-reading/" style="font-size: 20px;">ML paper-reading</a> <a href="/tags/Prof-Suresh/" style="font-size: 10px;">Prof.Suresh</a> <a href="/tags/sdh/" style="font-size: 10px;">sdh</a> <a href="/tags/summary/" style="font-size: 10px;">summary</a> <a href="/tags/suresh/" style="font-size: 10px;">suresh</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/09/18/talk-with-prof-suresh-3/">talk-with-prof-suresh-3</a>
          </li>
        
          <li>
            <a href="/2018/09/07/node-classification/">node classification, intro</a>
          </li>
        
          <li>
            <a href="/2018/09/04/talk-with-Prof-Suresh-2/">talk-with-Prof.Suresh-2</a>
          </li>
        
          <li>
            <a href="/2018/09/04/non-euclidean-data/">non-euclidean-data</a>
          </li>
        
          <li>
            <a href="/2018/09/03/sdh-workflow-investigation/">sdh-workflow-investigation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Zhou Zhe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>