<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Zhou Zhe&#39;s Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="Blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhou Zhe&#39;s Home">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Zhou Zhe&#39;s Home">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhou Zhe&#39;s Home">
  
    <link rel="alternate" href="/atom.xml" title="Zhou Zhe&#39;s Home" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhou Zhe&#39;s Home</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-node-classification" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/07/node-classification/" class="article-date">
  <time datetime="2018-09-07T16:22:41.000Z" itemprop="datePublished">2018-09-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/07/node-classification/">node classification, intro</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Node-Classification-or-Graph-Embedding"><a href="#Node-Classification-or-Graph-Embedding" class="headerlink" title="Node Classification or Graph Embedding"></a>Node Classification or Graph Embedding</h4><ul>
<li><a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">Inductive Representation Learning on Large Graphs</a>.</li>
</ul>
<h5 id="Goals-Encode-Decode-and-Data-Transformation"><a href="#Goals-Encode-Decode-and-Data-Transformation" class="headerlink" title="Goals: Encode, Decode and Data Transformation"></a>Goals: Encode, Decode and Data Transformation</h5><ul>
<li><p><strong>Distance</strong>: In the classical algorithm, if we can define a “distance” of two object, then we can classify them(such as K-means or others). For two high-dimensional vectors, the calculation of distance is easy, we have Euclidean distance or others. However, when the given data is a non-Euclidean data, especially a graph, we can not easily calculate the distance of two nodes. A reasonable definition of distance could be the length of a shortest route between two nodes. However, the complexity of calculating the shortest path between every two nodes is expensive. The traditional classification algorithm could not handle the non-Euclidean data.</p>
</li>
<li><p><strong>Embedding</strong>: In the view of math, a topology graph is a abstraction from a high-dimensional angle whose vertex is the origin of the coordinate(a graph with n node equal to a n-dimension object, which every node is located in one axis). To know this relation between the node of this angle, we do not need the information of the whole object. If we just project this object to a plane, we could also get the right distance. Converting a graph(is a high-dimensional object essentially) to a matrix(or a low-dimension vector) is called <em>Embedding</em>. Actually, the adjacency matrix is a method of embedding(because it loses a lot of information and the adjacency matrix itself is also complex).</p>
</li>
<li><p><img src="/images/embedding.jpeg" alt="embedding"></p>
</li>
<li><p><strong>Encode, Decode</strong>: The procedure for converting a graph to a matrix is called <em>encode</em>. We can also recover the graph by the encoded matrix, which is called <em>decode</em>. Then we can define a loss rate between the origin graph and “recovered graph”. If we use the <em>gradient descent</em> method to adjust the weight matrix to find the extremum of loss function, then we can find a reasonable encoder(because we would get a decode function, too. But it is useless at most of time).</p>
</li>
</ul>
<h5 id="From-NLP-to-Node-Analysis-Matrix-Factorization"><a href="#From-NLP-to-Node-Analysis-Matrix-Factorization" class="headerlink" title="From NLP to Node Analysis, Matrix Factorization"></a>From NLP to Node Analysis, Matrix Factorization</h5><ul>
<li><p>At first, the <em>Embedding</em> is used in NLP, which is called <em>word2vec</em>: <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a>. Word2vec use a <em>Skip-gram Model</em> which is a NN that has three layers: input, hidden and output. The input to hidden is the encoder, the hidden to output is the decoder. The forward of this NN is “encode + decode”, thus when we are training this NN, the encode is more and more reliable. After training, we extract the hidden layer, it is the resulting low-dimension matrix we want.</p>
</li>
<li><p>Then, this approach are used in the graph data which is called <em>DeepWalk</em>: <a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" target="_blank" rel="noopener">DeepWalk: Online Learning of Social Representations</a>, <em>node2vec</em>: <a href="https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf" target="_blank" rel="noopener">node2vec: Salable Feature Learning for Networks</a>. The different is the training data. In the NLP, the training data from M-gram, the traditional method. We first aggregate the adjacency word pairs(actually is the word pair which distance less than a hyper-parameter), and make sure the distance of represented vector of word pair is also close. In the graph, training data is the node pair which is in the neighborhood. But the neighborhood sample is more complex than NLP. The <em>DeepWalk</em> use sampling just merge <em>DFS</em> and <em>BFS</em>. <em>Node2vec</em> just improves the sampling.</p>
</li>
<li><p><img src="/images/DeepWalk.jpg" alt="DeepWalk"></p>
</li>
<li><p><img src="/images/Node2vec.jpg" alt="Node2vec"></p>
</li>
<li><p>the mathematical meaning of <em>node2vec</em> and <em>DeepWalk</em>. I have make a very simple explain before, and there is a paper(<a href="https://www.ijcai.org/Proceedings/15/Papers/299.pdf" target="_blank" rel="noopener">Network Representation Learning with Rich Text Information</a>) proof that <em>DeepWalk</em> is actually equivalent to <em>matrix factorization</em>. Let’s Image there is a <em>n x n</em> matrix _M_ correspond to a graph with _n_ nodes. If the value of <em>M(i,j)</em> is not zero, this indicate there is a path between node _i_ and node _j_ and the min length of the path is less than hyper-parameter _d_, which is just the depth of our <em>BNF</em> or <em>DNF</em>. If the <em>d = 1</em>, then the _M_ is our adjacency matrix. Then, this paper prove that <em>DeepWalk</em> just factorize this matrix, from <em>n x n</em> to <em>n x d</em>.</p>
</li>
<li><p><img src="/images/matrix-factorization.jpg" alt="matrix-factorization"></p>
</li>
</ul>
<h5 id="Graph-Convolutional-Networks-based-on-spectral"><a href="#Graph-Convolutional-Networks-based-on-spectral" class="headerlink" title="Graph Convolutional Networks(based on spectral)"></a>Graph Convolutional Networks(based on spectral)</h5><ul>
<li><p>If we have a graph structured of signals(a sequence of value), how can we find the hidden properties of these signals? The signals of the nodes are very important, but the relationship of the nodes is also meaningful.  Traditionally, machine learning handles this kind of problem by CNN, because convolution could <em>extract</em> hidden patterns of the data in the <em>neighborhood</em>. But how could we do convolution on a graph?</p>
</li>
<li><p>In the traditional convolution, we have a convolution <em>kernel</em>, which decides which information we should aggregate. These <em>kernels</em> are predefined, as during matrix convolution every <em>node</em> has 8 nodes surrounding it. However, the convolution kernels of the graph should come from the graph structure itself. If a node is 1-step nearby to you, you should aggregate the information of it. If a node is far from you, the corresponding value of the convolution kernel should be multiplied by zero.</p>
</li>
<li><p>The basic method to generate the kernel is <em>Graph Fourier Transform</em> which converts a adjacency matrix to a <em>kernel</em>. If the adjacency matrix is a positive symmetry matrix, it can be represented by the product between eigen-diag matrix and eigenvector. Then, the eigenvalues indicate the component of vector. In other words, if this node is close to one eigenvector this positive symmetry matrix is the <em>kernel</em> we want(more details in <a href="https://arxiv.org/pdf/1606.09375.pdf" target="_blank" rel="noopener">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>). <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a> introduces a simple and reasonable way to calculate the <em>kernel</em> from adjacency matrix. As we know, the <em>ReLU</em> part is the “encoder”, and <em>softmax</em> is the decoder.</p>
</li>
<li><p>This paper also use a <em>semi-supervised method</em> to train the NN. Not all the nodes have a label(such as, this node belongs to class A). Then, the error function only calculates the error of the labeled node. As the whole nodes shared the same weight matrix, we could also get the correct result.</p>
</li>
</ul>
<h5 id="Graph-Convolutional-Networks-based-on-spatial"><a href="#Graph-Convolutional-Networks-based-on-spatial" class="headerlink" title="Graph Convolutional Networks(based on spatial)"></a>Graph Convolutional Networks(based on spatial)</h5><ul>
<li><p>When we try to calculate a convolution, we actually first split the data into different parts, then times the <em>kernel</em> with each part and finally merge. So if we can find a reasonable way to split the graph into different but overlapping parts, the convolution in the graph is almost complete. Here we introduce two ways.</p>
</li>
<li><p>In the <a href="https://arxiv.org/pdf/1312.6203.pdf" target="_blank" rel="noopener">Spectral Networks and Deep Locally Connected Networks on Graphs</a>, the author uses a way to do convolution on graph-like data: each node has independent information vectors. First, the author uses the metric designed by <a href="http://robotics.stanford.edu/~ang/papers/nips11-SelectingReceptiveFields.pdf" target="_blank" rel="noopener">Selecting Receptive Fields in Deep Networks</a>, which is a statistical method to define the distance of two vectors. Then, we get a similarity matrix about each node pair. Then we could use a filter(in this paper, this is a threshold) to divide a neighborhood for each node. If there are some neighborhoods almost the same, then we randomly choose one. By using these nodes to neighborhoods mapping, we get a set of different but overlapping parts(if the number of nodes in the neighborhoods are not equal, add null nodes). If the neighborhood _NA_ and neighborhood _NB_ shared the same point from the original graph, then we could say there is a edge between _NA_ and _NB_. The neighborhoods actually have a graph-like structure. So the convolution could be recursive.</p>
</li>
<li><p>The disadvantage is that this adds a lot of null nodes, so the neighborhoods with less nodes would have higher priority. And the calculation complexity of <em>metric</em> make it not scalable to large graphs.</p>
</li>
<li><p><img src="/images/convolution-locally-on-graph.jpg" alt="convolution-locally-on-graph"></p>
</li>
<li><p>The author of <a href="https://arxiv.org/pdf/1605.05273.pdf" target="_blank" rel="noopener">Learning Convolutional Neural Networks for Graphs</a> provides another way. First, the author believes that not every node in the graph is really important; he suggests to choose the most important _n_ nodes, and do convolution on the neighborhoods of these nodes. So we should try to guarantee these _n_ neighborhood include all the nodes in the graph. The author ranks the nodes in the graph by the sum of distances from it to all other nodes. This approach is called <em>ranking</em>. After <em>ranking</em>, we extract the first _n_ nodes. For each node, he uses a <em>BFS</em> to construct the neighborhood. Then he <em>ranks</em> the neighborhoods, and sorts the neighborhoods. The sorted vector could be sent to do convolution now.</p>
</li>
<li><p><img src="/images/ranking-convolution-on-graph.jpg" alt="ranking-convolution-on-graph"></p>
</li>
<li><p>The complexity of this approach is also unscalable.</p>
</li>
</ul>
<h5 id="What-GraphSAGE-Do"><a href="#What-GraphSAGE-Do" class="headerlink" title="What GraphSAGE Do?"></a>What <em>GraphSAGE</em> Do?</h5><ul>
<li><p><strong>Shortage of old work</strong>:</p>
<ul>
<li>Focused on embedding nodes from a single fixed graph, but many real-world applications require embedding to be quickly generated for unseen nodes, or entirely new (sub)graphs. Both <em>Matrix Factorization</em> and <em>Graph Convolutional Networks</em> need a fixed abstraction of the adjacency matrix.</li>
<li>The weight matrices are not relevant between nodes in <em>Matrix Factorization</em>.</li>
<li><em>Graph-Based Neural Network</em> could not be used in large scale data. Because this approach requires to learn node degree-specific weight matrices does not scale to large graphs with wide node degree distributions.</li>
<li><em>Graph Convolutional Networks</em> is transductive node classification and does not naturally generalize to unseen nodes.</li>
</ul>
</li>
<li><p>If we cannot determine the graph structure because the structure is active, we should drop the idea of <em>matrix factor</em> or <em>convolution kernel</em>. We could try <em>DeepWalk</em>, just sample the neighborhood by iterating the graph. But there is a disadvantage of <em>DeepWalk</em>; the degree of graph is diverse. For the <em>DFS</em>, if node has large degrees it would generate more training data than others. This generates a kind of bias. To reduce this bias, we set a <em>specified degree</em> as a hyperparameter to restrict every degree of nodes to be the same.</p>
</li>
<li><p><img src="/images/GraphSAGE-algo.jpg" alt="GraphSAGE-algo"></p>
</li>
<li><p><img src="/images/forward-of-graphSAGE.jpg" alt="forward-of-graphSAGE"></p>
</li>
<li><p>Unlike <em>GCN</em>, the <em>GraphSAGE</em> is fully unsupervised. <em>GraphSAGE</em> aggregates the information of neighbor nodes, and every node shares the same weight matrix. <em>GraphSAGE</em> trusts that the information of nodes nearby is also similar. So, after forwarding of NN, the result we get is the low-dimension matrix. And we have the guarantee that each vector of node is close to its neighborhood.</p>
</li>
<li><p><img src="/images/error-function-graphSAGE.jpg" alt="error-function-graphSAGE"></p>
</li>
<li><p><strong>Aggregator</strong>. compare with <em>Graph Convolutional Networks</em>. If the aggregator is the <em>mean aggregator</em>, and the <em>GraphGAGE</em> does not distinguish old aggregation result and new information(the concatenation operation), they are the same. Another reasonable aggregator is <em>max pooling</em>(also the final choosing of the author), because it is both symmetric and trainable. Each neighbor’s vector is independently fed through a fully-connected neural network and then pooled. Then the model effectively captures different aspects of the neighborhood set.</p>
</li>
<li><p><strong>Minibatch</strong>. <em>GraphSAGE</em> also uses the minibatch method. For the set of all nodes in the graph, we can find a subset of the nodes: if we start from the nodes of the subset, we can go to every node in the graph in only one step. When we iterate this procedure, then we can get set list with size _d_, where _d_ is the max step we need to walk. Then, the difference set of one set of set list and its successor is the <em>batch of nodes</em> we want.</p>
</li>
<li><p><img src="/images/minibatch-of-graphSAGE.jpg" alt="minibatch-of-graphSAGE"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/07/node-classification/" data-id="cjlvn69p4000i2imc4599r6xd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML-paper-reading/">ML paper-reading</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-talk-with-Prof-Suresh-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/04/talk-with-Prof-Suresh-2/" class="article-date">
  <time datetime="2018-09-04T20:52:44.000Z" itemprop="datePublished">2018-09-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/04/talk-with-Prof-Suresh-2/">talk-with-Prof.Suresh-2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Start-of-workflow"><a href="#Start-of-workflow" class="headerlink" title="Start of workflow"></a>Start of workflow</h4><ul>
<li><em>19: Tokenization</em>.</li>
<li><em>46: Node Classification</em>.</li>
</ul>
<h4 id="Talk-summary"><a href="#Talk-summary" class="headerlink" title="Talk summary"></a>Talk summary</h4><ul>
<li>try to find the data properties of the <em>Tokenization</em> and <em>Node Classification</em>, just like we do in the <em>PageRank</em>: the graph is dense or sparse, how many node in the graph, how many edges in the graph, the underlying properties(some node are very close like a cluster, and the node between cluster are hardly have link).</li>
<li>According to these properties, design some algorithms and implementations for different properties.<ul>
<li>like there are so many punctuation(especaily the infix in the word) in the document, we can not cannot calculate it in parallel, as a word with a lot of punctuations could be a large graph, and this would delay the analize of whole document. So we have a data properties: rich-punctuation and barren-punctuation.</li>
<li>Algorithm.<ul>
<li>branch predict algorithm: predict there is a prefix or suffix in the word, and continue. If have a hazard, then handle it. This is fit the rich-punctuation document.</li>
<li>parallel algorithm: assign the word to the different <em>checker</em> in different thread. <em>cheacker</em> could search the database and return if the word in the database. The we merge the result of <em>checker</em>. This is fit the barren-punctuation document.</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/04/talk-with-Prof-Suresh-2/" data-id="cjlo6x998000d2i5www9ybnbc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prof-Suresh/">Prof.Suresh</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-non-euclidean-data" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/04/non-euclidean-data/" class="article-date">
  <time datetime="2018-09-04T19:12:25.000Z" itemprop="datePublished">2018-09-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/04/non-euclidean-data/">non-euclidean-data</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Geometric-deep-learning-going-beyond-Euclidean-data"><a href="#Geometric-deep-learning-going-beyond-Euclidean-data" class="headerlink" title="Geometric deep learning: going beyond Euclidean data"></a><a href="https://arxiv.org/pdf/1611.08097.pdf" target="_blank" rel="noopener">Geometric deep learning: going beyond Euclidean data</a></h4><ul>
<li><p>Euclidean data: have Euclidean properties. Like global parameterization, common system of coordinates, vector space structure, shift-invariance.</p>
<ul>
<li>Text sequence is Euclidean data. The _N_ is the set of word. the test sequence is one-dimension vector of _N_.</li>
<li>Image is Eucliean data. It is a 2-grid data.</li>
<li>Video is Eucliean data, it is a 3-grid data.</li>
</ul>
</li>
<li><p>non-Eucliean data: geometric like, graph.</p>
<ul>
<li>social network: people and people could construct a graph, then every vertex of the graph have a sequence of action. The history action of people is relevant with the other people’s action.</li>
<li>sensor networks: every sensor in the network has a sequence of signal.</li>
<li>3D-vision. The surface of 3D object are not Euclidean space, but we could cut every piece of surface to treat them as a manifold. Then we could use the CNN or other algorithm in these manifold.</li>
<li>recommander system(<em><a href="https://arxiv.org/pdf/1704.06803.pdf" target="_blank" rel="noopener">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</a></em>): the basic recommander system is a matrix, the one coordinate is user, the other is the object. The value of matrix is the score user give to the object. However, actually there is a graph connected the user, the edges indicate they have the same properties(age, gender, tendency…), on the other head, there is also a graph of objects. Then the two graph have some connect, these relationship has a projection in the 2-dimension matrix. Figure from <em><a href="https://arxiv.org/pdf/1704.06803.pdf" target="_blank" rel="noopener">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</a>:</em></li>
<li><img src="/images/recommander-system.jpg" alt="recommander-system"></li>
</ul>
</li>
<li><p>Some approach:</p>
<ul>
<li>Euclidean CNN vs. Geometric CNN.</li>
<li><img src="/images/Geometric-CNN.jpg" alt="Geometric-CNN"></li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/04/non-euclidean-data/" data-id="cjlo3ca2b00082i5wc4do75p9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML-paper-reading/">ML paper-reading</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-sdh-workflow-investigation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/03/sdh-workflow-investigation/" class="article-date">
  <time datetime="2018-09-03T12:00:31.000Z" itemprop="datePublished">2018-09-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/03/sdh-workflow-investigation/">sdh-workflow-investigation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="19-Tokenization"><a href="#19-Tokenization" class="headerlink" title="19: Tokenization"></a><em>19:</em> Tokenization</h4><ul>
<li>Simple, no AI. There is an <a href="https://spacy.io/usage/linguistic-features#how-tokenizer-works" target="_blank" rel="noopener">algorithm pseudo code</a> on spaCy website.</li>
<li>Basic procedure and paralleling.<ul>
<li>split and merge: split the text by spaces, handle each part of the text and merge. Could be done in parallel.</li>
<li>search in database: find if there are some special cases that match the text.</li>
<li>If the depth of the word is big, we could not run this in paralel.</li>
</ul>
</li>
</ul>
<h4 id="22-Named-Entity-Recognition"><a href="#22-Named-Entity-Recognition" class="headerlink" title="22: Named Entity Recognition"></a><em>22:</em> Named Entity Recognition</h4><ul>
<li>AI algorithm. There is an <a href="https://github.com/explosion/spaCy/issues/2107" target="_blank" rel="noopener">issue</a> in the GitHub repo that someone wanted to know the details about the paper of spaCy <em>NER</em>(Entity Recognition). Honnibal, the author of spaCy NLP lib, said:<br><em>There’s no paper published, as the algorithm wasn’t designed for an academic contribution, and details are subject to change. The overall approach is quite similar to the paper by Strubell et al (2017): <a href="https://arxiv.org/abs/1702.02098" target="_blank" rel="noopener">https://arxiv.org/abs/1702.02098</a> . The main differences are that we use a different embedding method, a transition-based framework to facilitate imitation learning, and the convolutional layers use residual connections instead of dilation.</em></li>
<li>this paper, <a href="https://arxiv.org/pdf/1702.02098.pdf" target="_blank" rel="noopener">Fast and Accurate Entity Recognition with Iterated Dilated Convolutions</a>, uses the iterated dilated CNN(authors of spaCy said they use ResNet instead).<ul>
<li>Use RNN instead of CNN(LSTMs), because RNN could use GPUs to accelerate.</li>
<li>Use fixed depth convolutions.</li>
<li>14-20x speedups over Bi-LSTM-CRF.</li>
<li>spaCy uses ResNet <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a>, <a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a>.</li>
</ul>
</li>
</ul>
<h4 id="46-Node-Classification"><a href="#46-Node-Classification" class="headerlink" title="46: Node Classification"></a><em>46:</em> Node Classification</h4><ul>
<li><a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">Inductive Representation Learning on Large Graphs</a>.<ul>
<li>Problem: node embedding in dynamic graph because ml needs a sequence of data. Additionally, the high-dimensional information about a node’s graph neighborhood is distilled into a dense vector embedding.</li>
<li>previous works:  focused on embedding nodes from a single fixed graph, but many real-world applications require embedding to be quickly generated for unseen nodes, or entirely new (sub)graphs. The weight matrices are not relevant between nodes.</li>
<li>contribution: simple neighborhood, aggregate feature information from neighbors.<ul>
<li>first construct a graph of the neighborhood with fixed num _n_ (if the number of the neighbors is greater than <em>a specified degree</em>, randomly drop some. if the number of the neighbors is less than <em>a specified degree</em>, randomly reuse some).</li>
<li><img src="/images/GraphSAGE-algo.jpg" alt="GraphSAGE-algo"></li>
<li>embedding generation. for depth k:<ul>
<li>for each node v:<ul>
<li>AGGREGATE v’s neighbors(mean, GCN, LSTM, pooling).</li>
<li>concat the the old aggregation of loop k-1 and current aggregation.</li>
<li>use non-linearity _σ_ to transform the concated vector for next step.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>update: the author just changed their words in <a href="https://github.com/williamleif/GraphSAGE/issues/13" target="_blank" rel="noopener">issue: Number nodes at each layers</a>. The code is correct.<ul>
<li>sample a “center” node, n0</li>
<li>sample 10 1-hop neighbors, n1</li>
<li>sample 25 1-hop neighbors of each n1, yielding 250 2-hop neighbors of n0</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="26-Recommender-System"><a href="#26-Recommender-System" class="headerlink" title="26: Recommender System"></a><em>26:</em> Recommender System</h4><ul>
<li><a href="https://github.com/amzn/amazon-dsstne" target="_blank" rel="noopener">Amazon DSSTNE: Deep Salable Sparse Tensor Network Engine</a><ul>
<li>Multi-GPU Scale: Training and prediction both scale out to use multiple GPUs, spreading out computation and storage in a model-parallel fashion for each layer.</li>
<li>Large Layers: Model-parallel scaling enables larger networks than are possible with a single GPU.</li>
<li>Sparse Data: DSSTNE is optimized for fast performance on sparse datasets, common in recommendation problems. Custom GPU kernels perform sparse computation on the GPU, without filling in lots of zeroes.</li>
</ul>
</li>
<li>I think the Recommender System is a 4 layer fullyconnnected NN from <a href="https://gitlab.sdh.cloud/sdh-workflows/amazon-dsstne/blob/master/_sdh/model.json" target="_blank" rel="noopener">DSSTNE model config file</a>.</li>
<li>As the materials I found, it basically is a <strong>Matrix completion problem:</strong> given a part of matrix, fill the rest. A Recommender system example is <em>Netflix</em>: In 2009, the Netflix challenge offered a 1M$ prize for the algorithm that can best predict user ratings for movies based on previous ratings. The size of the Netflix matrix is 480K movies and 18K users (8.5B elements), with only 0.011% known entries. There are some ml approaches: <a href="https://arxiv.org/pdf/1704.06803.pdf" target="_blank" rel="noopener">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</a>.</li>
</ul>
<h4 id="68-Changepoint-Detection"><a href="#68-Changepoint-Detection" class="headerlink" title="68: Changepoint Detection"></a><em>68:</em> Changepoint Detection</h4><ul>
<li><a href="https://arxiv.org/pdf/1209.1625.pdf" target="_blank" rel="noopener">Graph-Based Change-Point Detection</a>.<ul>
<li>Modern Statistical problem: every element of a sequence is a high-dimensional vector or even a non-Euclidean data object(graph like data). Example: Network evolution(<em>change in network of social interactions among individuals</em>), Image analysis(<em>The detection of abrupt events, such as security breaches, storms, or brain activity</em>), Text or sequence analysis(writing style,  genomics sequence analysis in biology).</li>
</ul>
</li>
</ul>
<h4 id="Resolve-order"><a href="#Resolve-order" class="headerlink" title="Resolve order"></a>Resolve order</h4><ul>
<li><strong>Tokenization</strong>(simple) -&gt; <strong>Node Classification</strong>(easy to understand, has paper support) -&gt; <strong>Changepoint Detection</strong>(has paper support) -&gt; <strong>Named Entity Recognition</strong>(easy to understand, but no direct paper support) -&gt; <strong>Recommender System</strong>(no paper support)</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/03/sdh-workflow-investigation/" data-id="cjlm8gysu00052imc094icou5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sdh/">sdh</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-reading-group-xy" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/31/reading-group-xy/" class="article-date">
  <time datetime="2018-08-31T19:06:18.000Z" itemprop="datePublished">2018-08-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/31/reading-group-xy/">reading_group_xy</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Reluplex-An-Efficient-SMT-Solver-for-Verifying-Deep-Neural-Networks"><a href="#Reluplex-An-Efficient-SMT-Solver-for-Verifying-Deep-Neural-Networks" class="headerlink" title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks"></a>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</h4><ul>
<li><p><a href="https://arxiv.org/pdf/1702.01135.pdf" target="_blank" rel="noopener">Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</a></p>
</li>
<li><p>To proof the NN is worked:</p>
<ul>
<li>In the past, we have some old algorithm besides deep learning, the reseachers design a very complex rule sets, which indicate what should a agent do when it in a situation. (A rule set restrict the input and output). But the data set is huge, and do bad when the agent meet the situation out of the rule set.</li>
<li>To improve this, we use the deep learning. The NN need a data set to train, and every instance of the data set just fellow this rules. But, also there is some undescovered rules are hidden in the data set. Then the NN could do the same things like the traditional algorithm, but also could handle the situation outside the rule sets.</li>
<li>However, we should prove that the NN do the same things like the old algorithm. And this is what it paper talking about. For each rule in the rule sets, we can convert it to a restriction to input and output. And this could be encode to a SMT problem, which could resolve by SMT solver.</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/31/reading-group-xy/" data-id="cjlidczfs00042i5vrofh5a23" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/31/2/" class="article-date">
  <time datetime="2018-08-31T16:57:03.000Z" itemprop="datePublished">2018-08-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/31/2/">Meeting in Aug 31, 2018</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Important-Points"><a href="#Important-Points" class="headerlink" title="Important Points"></a>Important Points</h4><ul>
<li>Workflow Assignment:<ul>
<li><a href="https://spacy.io/" target="_blank" rel="noopener"><em>19:</em> Tokenization</a>: Segment text from strings of characters/symbols into linguistic units like words and sentences.</li>
<li><a href="https://spacy.io/" target="_blank" rel="noopener"><em>22:</em> Named Entity Recognition</a>: Classify named entities in text into pre-defined categories such as names, organizations, locations, quantities, monetary values, etc.  Can include chunking of multiple word into a single entity.</li>
<li><a href="https://github.com/amzn/amazon-dsstne/" target="_blank" rel="noopener"><em>26:</em> Recommender System</a>: Recommend items to a user given the user’s ratings and a dataset of ratings from other users.  This version uses Amazon’s neural network based DSSTNE framework on the MovieLens dataset.</li>
<li><a href="https://gitlab.sdh.cloud/sdh-workflows/graphSAGE" target="_blank" rel="noopener"><em>46:</em> Node Classification</a>: Learns graph embedding to predict the ages of individuals based on the known ages of their friends.</li>
<li><a href="https://gitlab.sdh.cloud/sdh-workflows/changepoint" target="_blank" rel="noopener"><em>68:</em> Changepoint Detection</a>: Uses scan statistics to determine where the distribution abruptly changes in a data sequence.</li>
</ul>
</li>
</ul>
<h4 id="Next-Week-Works"><a href="#Next-Week-Works" class="headerlink" title="Next Week Works"></a>Next Week Works</h4><ul>
<li><p>Read these workflow briefly, get the general understanding. try to sort them, choose the first one to analyze.</p>
</li>
<li><p>Investigate the interface between Ocaml, Python and C. Learn some Ocaml and Haskell…</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/31/2/" data-id="cjli8qru500032i5vahy4anxk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-first-post" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/first-post/" class="article-date">
  <time datetime="2018-08-29T19:51:33.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/first-post/">Talk With Prof. Suresh in Aug 28, 2018</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h6 id="Works-Summary-of-Last-Week"><a href="#Works-Summary-of-Last-Week" class="headerlink" title="Works Summary of Last Week"></a>Works Summary of Last Week</h6><ul>
<li><strong>Clear the misunderstanding of approximate algorithm:</strong> the terminate state is not whole vertexs keep unchanged. We can tolerant a part of vertexs keep changing. So I introduce a new incoming argument _r_, which indicate the termination rate: if less than 1% of vertexs keep changing, then we can terminate the approximate algorithm.</li>
<li><strong>Fix the bug in kernel opencl:</strong> I had misused the <em>local memory</em> in opencl, now I have fixed it.</li>
<li><strong>Prepare the data set:</strong> I choose to use the format called <a href="https://math.nist.gov/MatrixMarket" target="_blank" rel="noopener">Matrix Market</a>. So I wrote a cpp code to transform the data format.</li>
<li><strong>Remove all dependency of Taco:</strong> The reason is that I find when <em>Taco</em> reading a matrix from disk, it try to malloc a huge memory and make a error. However, I think it should not use such memory, because for sparse matrix, the space to use is only related to the number of edges. Then, I want to rewrite the I/O part of <em>Taco</em>. But it bind tightly with the whole <em>data structure of tensor</em>. So I refactor the project and remove the all denpendency of <em>Taco</em>. Now, I design a tensor data structure by myself.</li>
</ul>
<h6 id="Talking-Content"><a href="#Talking-Content" class="headerlink" title="Talking Content"></a>Talking Content</h6><p><img src="/images/SDH.jpeg" alt="SDH" width="100%" height="100%"></p>
<h6 id="Works-Plan-of-Next-Week"><a href="#Works-Plan-of-Next-Week" class="headerlink" title="Works Plan of Next Week"></a>Works Plan of Next Week</h6><ul>
<li><strong>Improve the page rank algorithm:</strong> design the sparse version of approximate algorithm. which could make it run with huge data set(200k vertexs).</li>
<li><strong>Analyze the workflow:</strong> When DARPA decide three or four problem, we would try to analyze the problem. Maybe we would go to NEU.</li>
</ul>
<blockquote>
<p>The general information provided is expected to include identification of significant kernels/functions; algorithmic complexity; performance bottlenecks; description of data and any ETL requirements; opportunities for parallelization; and areas for optimization.  The performers are not expected to provide any information about performance of workloads on their proposed architectures.  If performance on an architecture is provided as part of the workflow analysis, it will be shared with the SDH community.</p>
</blockquote>
<ul>
<li><strong>Think about the procedure and data properties of page rank(and other problem): </strong> Deside use what kernel or algorithm by data properties in runtime.</li>
<li><strong>A result reusing problem:</strong> If we have got a partial result in one algorithm in one kernel, how can we transform the result to another algorithm in another kernel to resue this partial result.</li>
<li><strong>Preview the meta language:</strong> From Ocaml of Common Lisp.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/first-post/" data-id="cjlfk3gjb00002i4mr07rt8gf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/summary/">summary</a></li></ul>

    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML-paper-reading/">ML paper-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Prof-Suresh/">Prof.Suresh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sdh/">sdh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/summary/">summary</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML-paper-reading/" style="font-size: 20px;">ML paper-reading</a> <a href="/tags/Prof-Suresh/" style="font-size: 10px;">Prof.Suresh</a> <a href="/tags/sdh/" style="font-size: 10px;">sdh</a> <a href="/tags/summary/" style="font-size: 10px;">summary</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/09/07/node-classification/">node classification, intro</a>
          </li>
        
          <li>
            <a href="/2018/09/04/talk-with-Prof-Suresh-2/">talk-with-Prof.Suresh-2</a>
          </li>
        
          <li>
            <a href="/2018/09/04/non-euclidean-data/">non-euclidean-data</a>
          </li>
        
          <li>
            <a href="/2018/09/03/sdh-workflow-investigation/">sdh-workflow-investigation</a>
          </li>
        
          <li>
            <a href="/2018/08/31/reading-group-xy/">reading_group_xy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Zhou Zhe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>